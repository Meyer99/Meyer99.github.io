---
title: Coursera Deep Learning Specialization C1 Summary
subtitle: "Course 1: Neural Networks and Deep Learning"
layout: post
mathjax: true
author: Meyer
catalog: true
header-img: "img/brain.jpg"
tags: 
    - Coursera
    - Deep Learning
    - Summary

---

ðŸ‘‰[Link](https://www.coursera.org/learn/neural-networks-deep-learning) to the course

Big data makes deep learning taking off these years. This course introduces the neural networks from "shallow" to "deep". 
Week 1 is about course logistics and introduction.
Week 2 starts with viewing logistic regression as a neural network and talks about Python and vectorization. 
Week 3 and Week 4 introduce the architecture of neural networks and how to use backpropagation to train them.
The key points of this course are summarized as follows.

### Notation
Here is the [notation](https://d3c33hcgiwev3.cloudfront.net/_106ac679d8102f2bee614cc67e9e5212_deep-learning-notation.pdf?Expires=1581897600&Signature=QdE9eO59tatI4pBITG001BLIZuQRRk8oSOmQeAKbHj5Z4Tk1qDgfUyC2YnCNvArpLy5Zyz4uSl4vUsdufrvlsQAqtQ2lXUGl7eBYAtWB9-bChfqHSV85zbcTatH-eWDAGVf6-7ucTztRGTCLGue9nECP-XCILBLrRWVrxtYYqfk_&Key-Pair-Id=APKAJLTNE6QMUY6HBC5A) for representing neural networks throughout the course.

### Forward Propagation
Given the input $x$, we define $a^{[0]}=x$. Then for layer $\ell =1,2,...,L$, we have  
$1.\ z^{[\ell]}=W^{[\ell]}a^{[\ell-1]}+b^{[\ell]}$  
$2.\ a^{[\ell]}=g^{[\ell]}(z^{[\ell]})$  
Finally, given the output of the network $a^{[L]}=\hat{y}$, we measure the loss $J(W,b)=\mathcal{L}(\hat{y}, y)$.  
<br>
It generalizes when using mini-batch. Denote the batch size as $B$. Then, the input becomes $X=A^{[0]}=
\begin{bmatrix}
x^{(1)} & x^{(2)} & \cdots & x^{(B)}
\end{bmatrix}$. Similarly, we have  
$1.\ Z^{[\ell]}=W^{[\ell]}A^{[\ell-1]}+b^{[\ell]}$  (Note that $b^{[\ell]}$ is broadcasted)  
$2.\ A^{[\ell]}=g^{[\ell]}(Z^{[\ell]})$

### Backpropagation
We will define 

$$ \delta^{[\ell]}=\frac{\partial{\mathcal{L}}}{\partial{z^{[\ell]}}} $$

We can then define a three-step "recipe" for computing the gradients w.r.t. every $W^{[\ell]}$, $b^{[\ell]}$ as follows:  

(1) For output layer $\ell=L$, we compute $\delta^{[L]}$ directly (e.g., using computational graph when $g^{[L]}$ is the softmax function) or compute using the chain rule (e.g., when $g^{[L]}$ is the sigmoid function):

  $$ \delta^{[\ell]}=\frac{\partial{\mathcal{L}}}{\partial{a^{[\ell]}}} \cdot \frac{\partial{a^{[\ell]}}}{\partial{z^{[\ell]}}}=\frac{\partial{\mathcal{L}}}{\partial{a^{[\ell]}}}\circ (g^{[L]})'(z^{[L]})$$ 

  Note $(g^{[L]})'(z^{[L]})$ denotes the elementwise derivative w.r.t. $z^{[L]}$ and $\circ$ denotes the elementwise product.

(2) For $\ell=L-1, L-2, ..., 1$, we have 

$$ 
\begin{aligned}
\delta^{[\ell]}&=\frac{\partial{\mathcal{L}}}{\partial{z^{[\ell + 1]}}} \cdot \frac{\partial{z^{[\ell + 1]}}}{\partial{a^{[\ell]}}} \cdot \frac{\partial{a^{[\ell]}}}{\partial{z^{[\ell]}}}\\
&=(W^{[\ell+1]})^T\delta^{[\ell+1]}\circ (g^{[\ell]})'(z^{[\ell]})
\end{aligned}
$$ 

(3) Finally, for all $\ell$, we have

$$
\frac{\partial{\mathcal{L}}}{\partial{W^{[\ell]}}}=\frac{\partial{\mathcal{L}}}{\partial{z^{[\ell]}}}\cdot \frac{\partial{z^{[\ell]}}}{\partial{W^{[\ell]}}}=\delta^{[\ell]}(a^{[\ell-1]})^T
$$

$$
\frac{\partial{\mathcal{L}}}{\partial{b^{[\ell]}}}=\frac{\partial{\mathcal{L}}}{\partial{z^{[\ell]}}}\cdot \frac{\partial{z^{[\ell]}}}{\partial{b^{[\ell]}}}=\delta^{[\ell]}
$$

Similarly, when using mini-batch, we define

$$
\Delta^{[\ell]}=\frac{\partial{\mathcal{L}}}{\partial{Z^{[\ell]}}}
$$

and the cost function

$$
J=\frac{1}{B}\sum_{i=1}^B \mathcal{L}_i
$$


Then, we have

$$ 
\Delta^{[\ell]}=(W^{[\ell+1]})^T\Delta^{[\ell+1]}\circ (g^{[\ell]})'(Z^{[\ell]})
$$ 

$$
\frac{\partial{\mathcal{L}}}{\partial{W^{[\ell]}}}=\frac{1}{B}\Delta^{[\ell]}(A^{[\ell-1]})^T
$$

$$
\frac{\partial{\mathcal{L}}}{\partial{b^{[\ell]}}}=\frac{1}{B}np.sum(\Delta^{[\ell]}, axis=1,keepdims=True)
$$

### Miscellaneous
#### Random Initialization
It's important to initialize the weights randomly, one common way to do this is:  
`W = np.random.randn(W.shape[0], W.shape[1]) * 0.01`  
`b = np.zeros(b.shape)`

#### Activation Functions
Commonly used activation functions in neural networks:
* sigmoid: 

$$g(z)=\frac{1}{1+e^{-z}}$$

$$g'(z)=g(z)(1-g(z))$$

* tanh: 

$$g(z)=\frac{e^z-e^{-z}}{e^z+e^{-z}}$$

$$g'(z)=1-g(z)^2$$

* ReLU (Rectified Linear Unit):

$$g(z)=
\begin{cases}
z,& z\geq0\\
0,& z<0
\end{cases}
$$

$$
g'(z)=
\begin{cases}
1,& z\geq 0\\
0,& z<0
\end{cases}
$$

* Leaky ReLU:

$$
g(z)=
\begin{cases}
z,& z\geq0\\
az,& z<0
\end{cases}
$$

$$
g'(z)=
\begin{cases}
1,& z\geq 0\\
a,& z<0
\end{cases}
$$

where $a\in (0,1)$.


