---
title: Coursera Deep Learning Specialization C5W1 Summary
subtitle: "Week 1 of course Sequence Models"
layout: post
mathjax: true
author: Meyer
catalog: false
header-img: "img/cropped-index-2.png"
tags: 
    - Coursera
    - Deep Learning
    - Summary

---

ðŸ‘‰[Link](https://www.coursera.org/learn/nlp-sequence-models) to the course


* **Sequence model applications:**
  * Speech recognition
  * Music generation
  * Sentiment classification
  * DNA sequence analysis
  * Machine translation
  * Video activity recognition
  * Name entity recognition
* **Notation:**
  * Use $\langle t\rangle$ to index sequence position, e.g. $x^{\langle 1 \rangle}$, $y^{\langle t \rangle}$
  * Use $T_x$ and $T_y$ to denote the length of input sequence $x$ and output sequence $y$ respectively
* Why using RNN instead of a traditional neural network?
  * Inputs, outputs can be different lengths in different examples
  * Doesn't share features learned across different positions of text
* Words representation: use a one-hot vector to represent a word in the *vocabulary*
  * \<EOS\>: end of sentence
  * \<UNK\>: unknown word
* A language model estimates the probability of a given sentence: $P(y^{\langle 1 \rangle}, y^{\langle 2 \rangle}, \dots, y^{\langle T_y \rangle})$. A trained RNN can be used to sample sequence. 

![](/img/in-post/C5W1/RNN_language_model.jpg)

#### Forward and backward propagation in RNN
![](/img/in-post/C5W1/forward_prop.jpg)

Calculation for forward propagation: 

$$
\begin{aligned}
a^{\langle t \rangle} &= g_a(W_{aa}a^{\langle t-1 \rangle} + W_{ax}x^{\langle t \rangle} + b_a) \\
&= g_a(W_a[a^{\langle t-1 \rangle}, x^{\langle t \rangle}] + b_a)
\end{aligned} \\
\hat{y}^{\langle t \rangle} = g_y(W_{y}a^{\langle t \rangle} + b_y)
$$

*Backpropagation through time:*
![](/img/in-post/C5W1/bptt.jpg)

#### Different types of RNN
* many to many ($T_x\neq T_y$ and $T_x=T_y$)
* many to one (e.g. sentiment classification)
* one to many (e.g. music generation)
* one to one

![](/img/in-post/C5W1/RNN_types.jpg)

* The vanishing gradient problem happens more often when training RNN and is harder to solve compared to the exploding gradient problem. Specifically, `NaN` usually indicates that exploding gradient problem exists. To solve the exploding gradient problem, we can use a trick called *gradient clipping*. 

#### RNN, GRU & LSTM
* GRU and LSTM are proposed to help better capture the long-range connection and mitigate the vanishing gradient problem
* GRU and LSTM introduce the memory cell $c$
* GRU introduces two gates $\Gamma_u$ and $\Gamma_r$. The value of $c$ is maintained as long as $\Gamma_u\approx 0$.
* LSTM introduces three gates $\Gamma_u$, $\Gamma_f$ and $\Gamma_o$. Memory and input are added. The influence never disappears unless forget gate is *closed*. No gradient vanishing (if forget gate is *opened*).
* [Comparison of RNN/GRU/LSTM](http://dprogrammer.org/rnn-lstm-gru)
  
##### GRU

$$
\tilde{c}^{\langle t \rangle} = \tanh (W_c[\Gamma_r\odot c^{\langle t-1 \rangle}, x^{\langle t \rangle}] + b_c) \\
\Gamma_u=\sigma(W_u[c^{\langle t-1 \rangle}, x^{\langle t \rangle}]+b_u) \\
\Gamma_r=\sigma(W_r[c^{\langle t-1 \rangle}, x^{\langle t \rangle}]+b_r) \\
c^{\langle t \rangle} = \Gamma_u\odot\tilde{c}^{\langle t \rangle} + (1-\Gamma_u)\odot c^{\langle t-1 \rangle}\\
a^{\langle t \rangle} = c^{\langle t \rangle}
$$

##### LSTM

$$
\tilde{c}^{\langle t \rangle} = \tanh (W_c[a^{\langle t-1 \rangle}, x^{\langle t \rangle}] + b_c) \\
\Gamma_u=\sigma(W_u[a^{\langle t-1 \rangle}, x^{\langle t \rangle}]+b_u) \\
\Gamma_f=\sigma(W_f[a^{\langle t-1 \rangle}, x^{\langle t \rangle}]+b_f) \\
\Gamma_o=\sigma(W_o[a^{\langle t-1 \rangle}, x^{\langle t \rangle}]+b_o) \\
c^{\langle t \rangle} = \Gamma_u\odot\tilde{c}^{\langle t \rangle} + \Gamma_f\odot c^{\langle t-1 \rangle}\\
a^{\langle t \rangle} = \Gamma_o\odot\tanh(c^{\langle t \rangle})
$$

* **Bidirectional RNN (BRNN):** putting two independent RNNs together. The input sequence is fed in normal time order for one network, and in reverse time order for another. The outputs of the two networks are usually concatenated at each time step, though there are other options, e.g. summation.
![](/img/in-post/C5W1/BRNN.jpg)

* **Deep RNN**: Each hidden state is continuously passed to both the next timestep of the current layer and the current timestep of the next layer.
![](/img/in-post/C5W1/Deep_RNN.jpg)
