---
title: Coursera Deep Learning Specialization C2W1 Summary
subtitle: "Week 1 of course Improving Deep Neural Networks: Hyperparameter tuning, Regularization and Optimization - Practical aspects of Deep Learning"
layout: post
mathjax: true
author: Meyer
catalog: true
header-img: "img/brain.jpg"
tags: 
    - Coursera
    - Deep Learning
    - Summary

---

üëâ[Link](https://www.coursera.org/learn/deep-neural-network) to the course


### Setting up your Machine Learning Application
#### Train / Dev / Test sets
* Train set: data used to train the model
* Development set: a.k.a. hold-out cross validation set, used to evaluate the model's performance
* Test set: used for final estimation to get an unbiased estimate
* For a small amount of data, say, 100 - 10000, it's a common practice to 70/30 split into train/test set or 60/20/20 split into train/dev/test set. However, in the big data era, say 1 million, it's more than okay to have enough data for dev set and test set. 
* *Rule of thumb*: make sure dev set and test set come from the same distribution
* *Note*: not having a test set might be okay

#### Bias / Variance
* Bias and variance are manifested by the training set error and dev set error
* Suppose that the *optimal error* is 0%,  here are some possible cases:  

|                    | High variance | High bias | High variance  and high bias | Low variance and low bias |
|:------------------:|:-------------:|:---------:|:----------------------------:|:-------------------------:|
| Training set error |       1%      |    15%    |              15%             |            0.5%           |
|    Dev set error   |      11%      |    16%    |              30%             |             1%            |

#### Basic Recipe for Machine Learning
* Diagnose for the high bias problem first, followed by the high variance problem. 
* To solve the high bias problem:
  * Use bigger network
  * Train longer
  * Neural network architecture search
* To solve the high variance problem:
  * Fit more data
  * Apply regularization
  * Neural network architecture search
* Bias and variance can be driven down independently in modern deep-learning era, as long as we use bigger network with good regularization and fit more data.

### Regularizing your Neural Network
* L1 regularization: adds ‚Äúabsolute value of magnitude‚Äù of coefficient (i.e., L1 norm) as penalty term to the loss function
* L2 regularization (a.k.a. "weight decay"): adds ‚Äúsquared magnitude‚Äù of coefficient (i.e., L2 norm or Frobenius norm) as penalty term to the loss function
* Dropout regularization:
  * Eliminate some nodes in each layer of a neural network randomly
  * Implementation: "inverted dropout"
    * Only use dropout during training. Don't use dropout (randomly eliminate nodes) during test time.
    * Apply dropout both during forward and backward propagation.
    * During training time, divide each dropout layer by `keep_prob` to keep the same expected value for the activations.
  * Intuition: can't rely on any one feature, so have to spread out weights
  * Downside: the cost function is not well defined due to randomness, i.e. the cost function may not be monotonically decreasing during training
* Other regularization methods:
  * Data augmentation
  * Early stopping: stop training when dev set error starts to increase

### Setting up your Optimization Problem
* Normalizing the inputs can help speed up the training process. Conventionally, we calculate the mean and standard deviation on training set, and use those values to normalize both the training set and test set. 
* Deep neural networks often suffer from the vanishing/exploding gradients problem. A partial solution to this problem is careful weight initialization, of which the two common ways are:
  * $W^{[\ell]} = np.random.randn(W^{[\ell]}.shape) * \sqrt{\frac{2}{n^{[\ell-1]}}}$
  * $W^{[\ell]} = np.random.randn(W^{[\ell]}.shape) * \sqrt{\frac{2}{n^{[\ell-1]}+n^{[\ell]}}}$
* Gradient checking: for each parameter $\theta_i$, check whether the numerical approximation of gradient $\lim\limits_{\epsilon\to 0}\frac{J(\theta_i + \epsilon)-J(\theta_i -\epsilon)}{2\epsilon}$ is close to $\frac{\partial J}{\partial \theta_i}$ calculated by backpropagation
  * $difference = \frac {\mid\mid grad - gradapprox \mid\mid_2}{\mid\mid grad \mid\mid_2 + \mid\mid gradapprox \mid\mid_2} < 10^{-7}$