---
title: Coursera Deep Learning Specialization C5W3 Summary
subtitle: "Week 3 of course Sequence Models: Sequence models & Attention mechanism"
layout: post
mathjax: true
author: Meyer
catalog: true
header-img: "img/cropped-index-2.png"
tags: 
    - Coursera
    - Deep Learning
    - Summary

---

üëâ[Link](https://www.coursera.org/learn/nlp-sequence-models) to the course


### Various sequence to sequence architecture
* Machine translation

![](/img/in-post/C5W3/seq2seq_translation.jpg)

* Image captioning

![](/img/in-post/C5W3/image_captioning.jpg)

* Machine translation can be posed as a conditional language modeling problem, i.e. modeling the probability of output translation conditioned on the input sentence. To find the most likely translation, a greedy search (pick the most likely word at each timestep) usually doesn't work. Hence, we introduce the beam search algorithm, aiming to maximize that conditional probability. 

#### Beam search

Beam search is an improved algorithm based on greedy search. It has a hyper-parameter named beam size, $B$. At timestep 1, we select $B$ words with the highest conditional probability to be the first word of the $B$ candidate output sequences. For each subsequent timestep, we are going to select the $B$ output sequences with the highest conditional probability from the total of $B\|V\|$ possible output sequences based on the $B$ candidate output sequences from the previous timestep. These will be the candidate output sequences for that timestep. Finally, we will filter out the sequences containing the special symbol ‚Äú\<EOS\>‚Äù from the candidate output sequences of each timestep and discard all the subsequences after it to obtain a set of final candidate output sequences.

![](/img/in-post/C5W3/beam_search.jpg)


$$P(y^{\langle 1 \rangle}, y^{\langle 2 \rangle} | x) = P(y^{\langle 1 \rangle} | x)\cdot P(y^{\langle 2 \rangle} | x, y^{\langle 1 \rangle})$$

An illustration of beam search using beam size $B=2$:

![](/img/in-post/C5W3/beam_search2.jpg)


**Length normalization**

![](/img/in-post/C5W3/length_norm.jpg)

Multiplying the probability at each step may cause underflow. It's more stable to maximize the log of that probability. 
To prevent the algorithm tends to output short translation, we add a normalization term $\frac{1}{T_y^\alpha}$ where 
$\alpha$ is a hyperparameter whose value is between 0 and 1.

**Error analysis**

![](/img/in-post/C5W3/error_analysis.jpg)


#### BLEU Score

BLEU (bilingual evaluation understudy) is an algorithm for evaluating the quality of text which has been machine-translated from one natural language to another.  

BLEU score calculation example using bigram:

![](/img/in-post/C5W3/bleu_bigram.jpg)

BLEU score calculation details:

![](/img/in-post/C5W3/bleu_detail.jpg)

#### Attention Mechanism

The entire step-by-step process of applying Attention in [Bahdanau‚Äôs paper](https://arxiv.org/abs/1409.0473) is as follows:

*1.* Producing the encoder hidden states: encoder produces hidden states of each element in the input sequence  

*2.* Calculating alignment scores: using the previous decoder hidden state and each of the encoder‚Äôs hidden states to calculate

$$e^{\langle t,t' \rangle} = f(s^{\langle t-1 \rangle}, a^{\langle t' \rangle})$$

*3.* Softmaxing the alignment scores

$$\alpha^{\langle t,t' \rangle} = \frac{\exp{(e^{\langle t,t' \rangle})}}{\sum_{t'=1}^{T_x}\exp{(e^{\langle t,t' \rangle})}}$$

*4.* Calculating the context vector

$$c^{\langle t \rangle} = \sum_{t'=1}^{T_x}\alpha^{\langle t,t' \rangle}a^{\langle t' \rangle}$$

*5.* Decoding the output: the context vector is concatenated with the previous decoder output and fed into the Decoder RNN for that time step along with the previous decoder hidden state to produce a new output

*6.* The process (steps 2-5) repeats itself for each time step of the decoder until \<EOS\> is produced or output exceeds the specified maximum length

![](/img/in-post/C5W3/attention_model.jpg)

![](/img/in-post/C5W3/attention_weight.jpg)

Reference: <https://blog.floydhub.com/attention-mechanism>

### Speech recognition - Audio data
Speech recognition is the ability of a machine or program to identify words and phrases in spoken language and convert them to a machine-readable format.

![](/img/in-post/C5W3/speech_recognition.jpg)

The Attention model for speech recognition

![](/img/in-post/C5W3/speech_recognition1.jpg)

Connectionist temporal classification (CTC) is a type of neural network output and associated scoring function, for training recurrent neural networks (RNNs) such as LSTM networks to tackle sequence problems where the timing is variable. It can be used for tasks like on-line handwriting recognition or recognizing phonemes in speech audio. 

![](/img/in-post/C5W3/speech_recognition2.jpg)

Trigger word detection: a simple model architecture

![](/img/in-post/C5W3/trigger_word_detection.jpg)