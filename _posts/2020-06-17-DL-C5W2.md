---
title: Coursera Deep Learning Specialization C5W2 Summary
subtitle: "Week 2 of course Sequence Models: Natural Language Processing & Word Embeddings"
layout: post
mathjax: true
author: Meyer
catalog: true
header-img: "img/cropped-index-2.png"
tags: 
    - Coursera
    - Deep Learning
    - Summary

---

ðŸ‘‰[Link](https://www.coursera.org/learn/nlp-sequence-models) to the course


### Introduction to Word Embeddings
* One-hot vector representation cannot capture similarity between words. We introduce featurized representation, where each dimension embeds a feature. It generalizes better across different words. 
* Transfer learning and word embeddings:
  1. Learn word embeddings from large text corpus. (1-100B words) (Or download pre-trained embedding online)
  2. Transfer embedding to new task with smaller training set. (say, 100k words)
  3. Optional: Continue to finetune the word embeddings with new data.
* Word embeddings can help with analogy reasoning (e.g. figure out man to woman is like king to what)
* Embedding matrix is a $d$ by $\|V\|$ matrix in which $E\cdot o_j=e_j$
  * $o_j$ is the corresponding one-hot vector for word $j$
  * $e_j$ is the corresponding embedding for word $j$

### Learning Word Embeddings
**Neural language model**
  * Use a fixed length of words to predict the next word.
  * If you really want to build a language model, it's natural to use the last few words as a context. But if your main goal is really to learn a word embedding, then you can use other contexts and they will result in very meaningful word embeddings as well.

![](/img/in-post/C5W2/neural_language_model.jpg)

**Word2vec: Skip-Gram**
  * The model uses the current word to predict the surrounding window of context words.
  * Softmax function is computationally expensive (solution: hierarchical softmax)
  * Need to balance between common words and uncommon words while sampling context

![](/img/in-post/C5W2/skip-gram.jpg)

**Word2vec: CBOW**  
In the continuous bag-of-words architecture, the model predicts the current word from a window of surrounding context words. 

**Negative Sampling**  
Negative sampling modifies the original objective function. Instead of trying to predict the probability of being a nearby word for all the words in the vocabulary, we try to predict the probability that our training sample words are neighbors or not.

![](/img/in-post/C5W2/negative_sampling1.jpg)

So instead of having one giant softmax â€” classifying among 10,000 classes, we have now turned it into 10,000 binary classification problem. And on every iteration, we can only update weights associated with those $k+1$ words.

![](/img/in-post/C5W2/negative_sampling2.jpg)

**GloVe**  
$X_{ij}$ is the number of times word $j$ occurs in the context of word $i$.

![](/img/in-post/C5W2/GloVe.jpg)


### Dibiasing Word Embeddings
Word embeddings can reflect gender, ethnicity, age, sexual orientation, and other biases of the text used to train the model.

![](/img/in-post/C5W2/bias.jpg)

**Addressing bias in word embeddings**
1. Identify bias direction
2. Neutralize: for every word that is not definitional, project to get rid of bias
3. Equalize pairs

![](/img/in-post/C5W2/addressing_bias.jpg)