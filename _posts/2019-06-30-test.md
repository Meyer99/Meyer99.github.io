---
title: Test
layout: post
mathjax: true
author: Meyer
tags: 
    - TeXt

---


# CS 229, Public Course

## Problem Set #1: Supervised learning

### ***1. Newton's method for computing least squares***  

**Solution:**  
*(a)*  
Given the cost function $J(\theta)=\frac{1}{2}\sum_{i=1}^n(\theta^Tx^{(i)}-y^{(i)})^2$, we can easily derive that

$$
\nabla_{\theta}J(\theta)=\sum_{i=1}^n{(\theta^Tx^{(i)}-y^{(i)})}x^{(i)}
$$
The Hessian of the cost function is

$$
H=\frac{\partial\nabla_{\theta}J(\theta)}{\partial\theta}=\sum_{i=1}^n{x^{(i)}(x^{(i)})^T}
$$

*(b)*  
Newton's method update rule:

$$
\theta:=\theta-H^{-1}\nabla_{\theta}J(\theta)
$$
Observe that the normal equation solution gives us $\theta^*=(X^TX)^{-1}X^T\vec y$ and that

$$
X^TX=
\begin{bmatrix}
x^{(1)}&x^{(2)}&\cdots&x^{(n)}
\end{bmatrix}
\begin{bmatrix}
(x^{(1)})^T\\
(x^{(2)})^T\\
\vdots\\
(x^{(n)})^T\\
\end{bmatrix}
=\sum_{i=1}^n{x^{(i)}(x^{(i)})^T}=H
$$

Expressing the gradient in matrix-vector notation yields  

$$
\begin{aligned}
\nabla_{\theta}J(\theta)&=\sum_{i=1}^n{\theta^Tx^{(i)}x^{(i)}-\sum_{i=1}^n{y^{(i)}x^{(i)}}}\\
&=\begin{bmatrix}
x^{(1)}&x^{(2)}&\cdots&x^{(n)}
\end{bmatrix}
\begin{bmatrix}
(x^{(1)})^T\theta\\
(x^{(2)})^T\theta\\
\vdots\\
(x^{(n)})^T\theta\\
\end{bmatrix}-
\begin{bmatrix}
x^{(1)}&x^{(2)}&\cdots&x^{(n)}
\end{bmatrix}
\begin{bmatrix}
y^{(1)}\\
y^{(2)}\\
\vdots\\
y^{(n)}
\end{bmatrix}\\
&=X^TX\theta-X^T\vec y
\end{aligned}
$$

Therefore, no matter what initial value of $\theta^{(0)}$ is, after one iteration, we get

$$
\begin{aligned}
\theta^{(1)}&=\theta^{(0)}-(X^TX)^{-1}\nabla_{\theta}J(\theta)\bigg|_{\theta=\theta^{(0)}}\\
&=\theta^{(0)}-(X^TX)^{-1}(X^TX\theta^{(0)}-X^T\vec y)\\
&=\theta^{(0)}-\theta^{(0)}+(X^TX)^{-1}X^T\vec y\\
&=(X^TX)^{-1}X^T\vec y\\
&=\theta^*
\end{aligned}
$$
<br>
**Notes:**  
Using matrix-vector notation to express the gradient of the cost function is the key to this problem. By doing so, we are given a more concise form of the cost function and new insight. One may inspired to do so by noticing the similarities in the updating rule and $\theta^*$.

### ***3. Multivariate least squares***  

**Solution:**  
*(a)*  
In this case,

$$J(\Theta)=\frac{1}{2}\sum_{i=1}^n\sum_{j=1}^p\left((\Theta^Tx^{(i)})_j-y_j^{(i)}\right)^2
$$
where $\Theta\in\mathbb{R}^{(d+1)\times p}$.  
Note that the predicted value $y=\Theta^Tx$, so $y^T=x^T\Theta$. Stacking up the input and output variables:

$$
X\Theta-Y=
\begin{bmatrix}
(x^{(1)})^T\\
(x^{(2)})^T\\
\vdots\\
(x^{(n)})^T\\
\end{bmatrix}
\Theta-
\begin{bmatrix}
(y^{(1)})^T\\
(y^{(2)})^T\\
\vdots\\
(y^{(n)})^T\\
\end{bmatrix}
$$
The $(i, j)$th entry of matrix $X\Theta-Y$ is $(\Theta^Tx^{(i)})_j-y_j^{(i)}$.  
Therefore, the cost function is given by 

$$
J(\Theta)=\frac{1}{2}\text{tr}((X\Theta-Y)^T(X\Theta-Y))
$$

## 试一下中文的标题标题标题标题标题
### 试一下中文的标题标题标题标题标题
