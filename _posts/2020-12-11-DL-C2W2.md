---
title: Coursera Deep Learning Specialization C2W2 Summary
subtitle: "Week 2 of course Improving Deep Neural Networks: Hyperparameter tuning, Regularization and Optimization - Optimization algorithms"
layout: post
mathjax: true
author: Meyer
catalog: true
header-img: "img/brain.jpg"
tags: 
    - Coursera
    - Deep Learning
    - Summary

---

ðŸ‘‰[Link](https://www.coursera.org/learn/deep-neural-network) to the course


**Choosing mini-batch size**
* For a small training set (say, #training examples $\leq 2000$), use batch gradient descent
* Typical mini-batch size: 64, 128, 256, 512; make sure they fit in CPU/GPU memory
  

**Exponentially weighted average**
* $V_t = \beta V_{t-1} + (1-\beta)\theta_t, V_0=0$
* $V_t$ can be viewed as approximately averaging over the previous $\frac{1}{1-\beta}$ samples
* Bias correction: since $V_0=0$, the estimates during the initial phase is not accurate. One can instead use $\frac{V_t}{1-\beta^t}$ as a debiased estimate.

### Gradient descent with momentum
The momentum algorithm accumulates an exponentially decaying moving average of past gradients and continues to move in their direction.

* $V_{dW} = \beta V_{dW} + (1-\beta)dW$
* $V_{db} = \beta V_{db} + (1-\beta)db$
* $W = W - \alpha V_{dW}, \  b = b-\alpha V_{db}$
* $\beta=0.9$ is a common choice

![](/img/in-post/C2W2/SGDM.jpg)

Algorithm description from the book [*deep learning*](https://www.deeplearningbook.org/): (which is slightly different from the one described above)

![](/img/in-post/C2W2/SGDM_alg.jpg)

*Example usage in tensorflow keras:*
```py
opt = tf.keras.optimizers.SGD(learning_rate=0.1, momentum=0.9, nesterov=False)
```

### AdaGrad & RMSProp

The AdaGrad algorithm individually adapts the learning rates of all model parameters by scaling them inversely proportional to the square root of the sum of all the historical squared values of the gradient. The RMSProp algorithm modifies AdaGrad to perform better in the nonconvex setting by changing the gradient accumulation into an exponentially weighted moving average.

![](/img/in-post/C2W2/RMSprop.jpg)

![](/img/in-post/C2W2/RMSProp_alg.jpg)


*Example usage in tensorflow keras:*
```py
opt = tf.keras.optimizers.Adagrad(learning_rate=0.001)

opt = tf.keras.optimizers.RMSprop(learning_rate=0.001, rho=0.9)
```

### Adam
Adam optimization is a stochastic gradient descent method that is based on adaptive estimation of first-order and second-order moments, which can be seen as a variant on the combination of RMSProp and momentum.

![](/img/in-post/C2W2/Adam_alg.jpg)

*Example usage in tensorflow keras:*
```py
opt = tf.keras.optimizers.Adam(learning_rate=0.001, beta_1=0.9, beta_2=0.999)
```

**Learning Rate Decay**  
To reduce the learning rate over time. An initially large learning rate accelerates training or helps the network escape spurious local minima. Decaying the learning rate helps the network converge to a local minimum and avoid oscillation.

**The problem of local optima**  
![](/img/in-post/C2W2/local_optima.jpg)
* Unlikely to get stuck in a bad local optimum if we are training a reasonably large neural network (a saddle point is more likely)

![](/img/in-post/C2W2/plateau.jpg)
* Plateaus can make learning slow

***Reference***  
The [*Deep Learning*](https://www.deeplearningbook.org/) book by Ian Goodfellow and Yoshua Bengio and Aaron Courville.