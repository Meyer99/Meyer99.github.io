---
title: Coursera Deep Learning Specialization C2W2 Summary
subtitle: "Week 2 of course Improving Deep Neural Networks: Hyperparameter tuning, Regularization and Optimization - Optimization algorithms"
layout: post
mathjax: true
author: Meyer
catalog: true
header-img: "img/brain.jpg"
tags: 
    - Coursera
    - Deep Learning
    - Summary

---

ðŸ‘‰[Link](https://www.coursera.org/learn/deep-neural-network) to the course


**Choosing mini-batch size**
* For a small training set (say, #training examples $\leq 2000$), use batch gradient descent
* Typical mini-batch size: 64, 128, 256, 512; make sure they fit in CPU/GPU memory
  

**Exponentially weighted average**
* $V_t = \beta V_{t-1} + (1-\beta)\theta_t, V_0=0$
* $V_t$ can be viewed as approximately averaging over the previous $\frac{1}{1-\beta}$ samples
* Bias correction: since $V_0=0$, the estimates during the initial phase is not accurate. One can instead use $\frac{V_t}{1-\beta^t}$ as an debiased estimate.

**Gradient descent with momentum**
* $V_{dW} = \beta V_{dW} + (1-\beta)dW$
* $V_{db} = \beta V_{db} + (1-\beta)db$
* $W = W - \alpha V_{dW}, \  b = b-\alpha V_{db}$
* $\beta=0.9$ is a common choice


